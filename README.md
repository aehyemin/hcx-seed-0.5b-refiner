# HyperCLOVA SEED X DPO 학습용 데이터 설계


## 1. 프로젝트 개요

소형 LLM(HyperCLOVA SEED X 0.5B)의 정보 부족 상황에서의 환각 억제를 목표로
DPO(Direct Preference Optimization) 학습용 데이터셋을 설계하고 Full Fine-tuning으로 학습했습니다.



## 2. 문제 정의
정보가 충분하지 않은 질문에 대해 소형 모델은 다음과 같은 문제를 보였습니다:
- 근거 없이 단정하는 응답 생성
- 존재하지 않거나 답이 아닌 날짜/수치 생성

모르는 질문에 대해 모르거나 정보가 부족하다고 대답할 수 있도록 하는 것을 목표로 설정했습니다.

## 3. 데이터 설계 전략 

### 3.1 질문 설계

- 정보 부족 유형 시드 15개 정의
- Gemini API를 활용해 각 질문을 40개씩 증강

### 3.2 Preference Pair 구성

| 역할 | 구성 방식 |
|------|------------|
| Chosen | Gemini가 생성한 응답 |
| Rejected | 0.5B 모델이 생성한 응답 |

---

## 4. 학습 전략 비교

### 4.1 LoRA Fine-tuning

- WandB 지표상 개선
- 실제 추론 응답 변화 매우 적음

### 4.2 Full Fine-tuning 전환

- 모델 전체 가중치 재조정
- 추론 응답 변화 일부 존재
- 여전히 환각 잔존

## 5. 평가

| 추론 환경 | 결과 |
|------------|--------|
| Greedy | 환각 감소 |
| Sampling | 확정적 환각 잔존 |

## 6. Key Insights
본 프로젝트를 통해 데이터의 다양성과 대조성(Contrast)이 모델 정렬에 미치는 결정적 영향을 확인했습니다.

### 6.1. 비일관적인 Rejected 데이터의 학습 효과 저해
현상: 0.5B 모델이 생성한 오답이 질문과 무관한 지식 파편(예: 닭고기 요리법 등)을 포함한 '비일관적인 노이즈'에 가까웠습니다.

한계: 오답의 논리가 결여되어 정답과의 차이가 너무 자명했기 때문에, 모델이 정교한 **선호 경계**를 학습하기 위한 유의미한 대조군 역할을 수행하지 못했습니다.

### 6.2. Chosen 답변의 다양성 부족에 따른 패턴 매칭
현상: 모든 거절 응답이 특정 템플릿(예: "정보 부족으로 알 수 없습니다")으로 고착되었습니다.

한계: 모델이 "언제 거절해야 하는가"라는 추론 로직을 익히기보다, 특정 문구 패턴만 반복 출력하는 '패턴 매칭' 방향으로 수렴하는 경향을 보였습니다.

### 6.3. 결론: 데이터셋 설계의 핵심, '전략적 대조'
성능 개선의 핵심은 단순히 고품질 정답을 보여주는 것이 아니라, '피해야 할 오답'의 기준을 얼마나 날카롭고 일관되게 정의하느냐에 있음을 확인했습니다.

향후 고도화를 위해서는 Chosen 답변의 유형을 다변화하고, 모델이 실제 추론 과정에서 혼동할 수 있는 수준의 전략적 Rejected 데이터 설계가 선행되어야 함을 시사합니다.
